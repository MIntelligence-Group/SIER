Interpretable Emotion Recognition using Hybrid Fusion of Multimodal Speech and Image Signals
============================================================================================

Implementation for the paper (submitted to IEEE Signal Processing Letters Journal). <br>
**[Interpretable Emotion Recognition using Hybrid Fusion of Multimodal Speech and Image Signals][1]**<br>
[Puneet Kumar](https://puneet-kr.github.io/), Sarthak Malik and [Balasubramanian Raman](http://faculty.iitr.ac.in/~balarfma/)  

## Code Files
The code files are currently private as the corresponding research paper in ICASSP 2022 is under review. They will be made publically available soon after the paper is published/accepted for publication.

Dataset Access
--------------
Access to the ‘IIT Roorkee Speech and Image Emotion Recognition (IIT-R SIER) dataset’ can be obtained by through [`Access Form - IIT-R SIER Dataset.pdf`][2]. The dataset is prepared by Puneet Kumar and Sarthak Malik at Machine Intelligence Lab, IIT Roorkee under the supervision of Prof. Balasubramanian Raman. It contains speech utterances, corresponding images and emotion labels (happy, sad, hate, anger).

[1]: https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=97
[2]:https://github.com/MIntelligence-Group/SIER/blob/main/Access%20Form%20-%20IIT-R%20SIER%20Dataset.pdf 
