Interpretable Multimodal Emotion Recognition using Hybrid Fusion of Speech and Image Data
============================================================================================

Implementation for the paper (submitted to Springer Multimedia Tools and Applications (MTAP) Journal). <br>
**[Interpretable Multimodal Emotion Recognition using Hybrid Fusion of Speech and Image Data][1]**<br>
[Puneet Kumar](https://puneet-kr.github.io/), Sarthak Malik and [Balasubramanian Raman](http://faculty.iitr.ac.in/~balarfma/)  

## Code Files
The code files were private till the corresponding research paper's acceptance in Springer MTAP. They will be made publically available soon.

Dataset Access
--------------
Access to the ‘IIT Roorkee Speech and Image Emotion Recognition (IIT-R SIER) dataset’ can be obtained by through [`Access Form - IIT-R SIER Dataset.pdf`][2]. The dataset is prepared by Puneet Kumar and Sarthak Malik at Machine Intelligence Lab, IIT Roorkee under the supervision of Prof. Balasubramanian Raman. It contains speech utterances, corresponding images and emotion labels (happy, sad, hate, anger).

[1]:https://www.journals.elsevier.com/pattern-recognition-letters  
[2]:https://github.com/MIntelligence-Group/SIER/blob/main/Access%20Form%20-%20IIT-R%20SIER%20Dataset.pdf 
